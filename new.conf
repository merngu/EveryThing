#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

######
###### This config file is a demonstration of batch processing in SeaTunnel config
######

env {
  # You can set spark configuration here
  # see available properties defined by spark: https://spark.apache.org/docs/latest/configuration.html#available-properties
  spark.app.name = "ST-hive2ck-${table}-"${day}""
  spark.executor.instances = ""${exe_num}""
  spark.executor.cores = ""${exe_cores}""
  spark.executor.memory = ""${exe_mem}""
  spark.driver.memory = ""${drv_mem}""
  spark.driver.cores = ""${drv_cores}""
  spark.sql.catalogImplementation = "hive"
  spark.dynamicAllocation.enabled = false
  spark.locality.wait = 60000
  spark.serializer=org.apache.spark.serializer.KryoSerializer
  spark.yarn.queue=""${queue}""
  spark.driver.maxResultSize=10g
  spark.sql.hive.caseSensitiveInferenceMode=NEVER_INFER
}

source {
  # This is a example input plugin **only for test and demonstrate the feature input plugin**
  hive {
   pre_sql = "SELECT * from ${table}  where day='"${day}"'"
    result_table_name = "my_dataset"
  }
}

transform {
  # you can also use other filter plugins, such as sql
  # sql {
  #   sql = "select * from accesslog where request_time > 1000"
  # }
}

sink {
 clickhouse {
    host = "bi-clickhouse3.gitv.we:8123,bi-clickhouse4.gitv.we:8123"
    # 这个模式仅支持分布式引擎的clickhouse表.并且 internal_replication 参数必须为 true.往分布式表写数据开启自动分片模式，将在 seatunnel 中拆分分布式表数据，并直接在每个分片上执行写入
    split_mode= true
    # 使用split_mode时，向哪个节点发送数据是一个问题，默认为随机选择，但'sharding_key'参数可用于指定分片算法的字段。此选项仅在“split_mode”为真时才有效
    #sharding_key = ""
    clickhouse.socket_timeout = 50000
    database = "bdb"
    table = "${table}"
    username = "developer"
    password = "cqKUkv!#IAvAOkbt("
    bulk_size = 500000
    retry_codes = [209, 210]
    retry = 3
 }
}
